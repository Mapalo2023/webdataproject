{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7viebg5G6Q6n"
   },
   "source": [
    "<h1><center>Project 2: Working with Web Data</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP93pPu66hN5"
   },
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqMhsPlR6eC9"
   },
   "source": [
    "<h2><center>Mapalo Lukashi, Student ID: 800759428</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_fkDVH96qhp"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Part 1: XML Sitemap Parsing](#first-bullet)\n",
    "* [Part 2: Using an API](#second-bullet)\n",
    "* [Part 3: Web Scraping](#third-bullet)\n",
    "* [Part 4: Analysis of Part 3 data](#fourth-bullet)\n",
    "* [Conclusion](#fifth-bullet)\n",
    "* [References & Appendix](#sixth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7NWshvI7C33"
   },
   "source": [
    "## PART 1: XML Sitemap Parsing <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFnaKx217Nls"
   },
   "source": [
    "**Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pLtnccn7Ytx"
   },
   "source": [
    "The aim of this part was to develop a Python class capable of fetching and parsing the sitemap data from the Forbes website ('forbes.com'). This involved processing the 'robots.txt' file to identify sitemap URLs and then extracting information from these sitemaps into a structured DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD5u3Ggd7rJa"
   },
   "source": [
    "**Methodology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_SGopBe7-k-"
   },
   "source": [
    "The following steps were undertaken to achieve the project's objectives:\n",
    "\n",
    "**1. Identifying Sitemap URLs**\n",
    "\n",
    "> I began by accessing the 'robots.txt' file of the Forbes website. This file, available at https://www.forbes.com/robots.txt, provided several sitemap URLs. These URLs pointed to XML sitemaps, some of which were compressed using gzip ('.gz' extension).\n",
    "\n",
    "**2. Python Class Development**\n",
    "\n",
    "> A Python class named ForbesSitemapParser was created with the following key functionalities:\n",
    "\n",
    "> Fetching 'robots.txt': The class includes a method to request and read the contents of the 'robots.txt' file.\n",
    "Extracting Sitemap URLs: A method was implemented to parse the 'robots.txt' content and extract all listed sitemap URLs.\n",
    "Downloading and Parsing Sitemaps: The class handles the downloading of sitemap files.\n",
    "Data Extraction: Relevant data from each sitemap entry, such as URL and last modified date, is extracted and stored.\n",
    "DataFrame Compilation: Extracted data is compiled into a pandas DataFrame for easy analysis and manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ForbesSitemapParser class is initialized with the base URL of the Forbes website. It then processes the sitemaps and compiles the data into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sitemap Report for Forbes\n",
      "--------------------------------------\n",
      "Total URLs found: 2189\n",
      "                                                urls\n",
      "0  https://www.forbes.com/sites/forbes-personal-s...\n",
      "1  https://www.forbes.com/sites/hughmcintyre/2023...\n",
      "2  https://www.forbes.com/sites/truetamplin/2023/...\n",
      "3  https://www.forbes.com/sites/anneeaston/2023/1...\n",
      "4  https://www.forbes.com/sites/forbes-personal-s...\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class from my package\n",
    "from my_webdatapack.ForbesSitemapParser import ForbesSitemapParser\n",
    "\n",
    "parser = ForbesSitemapParser()\n",
    "df = parser.get_sitemap_data()\n",
    "print(\"Sitemap Report for Forbes\")\n",
    "print(\"--------------------------------------\")\n",
    "print(f\"Total URLs found: {len(df)}\")\n",
    "if not df.empty:\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"No URLs found in the sitemap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Description and Potential Uses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Description: The dataset generated from this processis a list of URLs found in Forbes' sitemaps. Each row in the DataFrame represents a different URL from the website.\n",
    "\n",
    "Potential Uses:\n",
    "\n",
    "* SEO Analysis: Analyzing the structure of the website and how content is indexed by search engines.\n",
    "* Content Scraping: Gathering URLs for scraping website content, respecting robots.txt guidelines.\n",
    "* Site Structure Analysis: Understanding the organization and hierarchy of content on Forbes.com.\n",
    "* Change Tracking: Monitoring the sitemap over time to detect new or removed pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using an API <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    ">The realm of culinary exploration and recipe management has witnessed a surge in the demand for programmatic access to vast databases of recipes and their detailed information. To address this need, I have developed a Python class, RecipeDataCollector, which interfaces with the MealDB API. This tool enables users to efficiently fetch comprehensive recipe information and extract ingredients from recipe texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MealDB API Overview**\n",
    "\n",
    "> The MealDB API is a comprehensive repository of culinary data, providing access to a vast collection of recipes from various cuisines around the world. It offers functionalities such as searching for recipes by name or ingredient, filtering by category or area, and retrieving detailed information about individual recipes. For the scope of this project, the focus was narrowed to fetching recipe information, extracting ingredients from recipe texts, and exploring additional API features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Description**\n",
    "\n",
    "> The data retrieved through the MealDB API encompasses: https://www.themealdb.com/api.php\n",
    "\n",
    "**Recipe Information:**\n",
    "\n",
    "* Recipe ID: A unique identifier for each recipe.\n",
    "* Name: Title of the recipe.\n",
    "* Area: Origin of the recipe.\n",
    "* Category: Culinary classification of the recipe.\n",
    "* Instructions: Step-by-step preparation guide.\n",
    "* Image URL: Link to the recipe's image.\n",
    "\n",
    "**Ingredients Extraction:**\n",
    "\n",
    "* Ingredient Name: The name of the ingredient.\n",
    "* Ingredient Quantity: Amount of the ingredient.\n",
    "* Ingredient Measure: Measurement unit for the ingredient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "\n",
    "> Python Class: RecipeDataCollector\n",
    "\n",
    "> The class was designed with the following features:\n",
    "\n",
    "* Initialization: Takes an API URL as a parameter and stores it for subsequent API calls.\n",
    "\n",
    "* Search by Name: Retrieves a list of recipes matching the specified search term.\n",
    "\n",
    "* List by First Letter: Retrieves a list of recipes starting with the given letter.\n",
    "\n",
    "* Lookup by ID: Retrieves detailed information about a recipe given its ID.\n",
    "\n",
    "* Get Random Recipe: Retrieves a random recipe from the database.\n",
    "\n",
    "* List All Categories: Retrieves a list of all available recipe categories.\n",
    "\n",
    "> The class utilizes requests for HTTP communication and pandas to structure the returned JSON data into a DataFrame, making it conducive for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary class from my package\n",
    "from my_webdatapack.RecipeDataCollector import RecipeDataCollector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes containing 'pasta':\n",
      "      id                       name     area category  \\\n",
      "0  52777  Mediterranean Pasta Salad  Italian  Seafood   \n",
      "\n",
      "                                        instructions  \n",
      "0  Bring a large saucepan of salted water to the ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the API URL\n",
    "api_url = \"https://www.themealdb.com\"\n",
    "\n",
    "# Create an instance of the RecipeDataCollector class\n",
    "recipe_collector = RecipeDataCollector(api_url)\n",
    "\n",
    "# Search for recipes containing \"pasta\"\n",
    "pasta_recipes_df = recipe_collector.search_by_name(\"pasta\")\n",
    "print(\"Recipes containing 'pasta':\")\n",
    "print(pasta_recipes_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes starting with 'a':\n",
      "      id                        name       area category  \\\n",
      "0  52768        Apple Frangipan Tart    British  Dessert   \n",
      "1  52893  Apple & Blackberry Crumble    British  Dessert   \n",
      "2  53049                  Apam balik  Malaysian  Dessert   \n",
      "3  53050                 Ayam Percik  Malaysian  Chicken   \n",
      "\n",
      "                                        instructions  \n",
      "0  Preheat the oven to 200C/180C Fan/Gas 6.\\r\\nPu...  \n",
      "1  Heat oven to 190C/170C fan/gas 5. Tip the flou...  \n",
      "2  Mix milk, oil and egg together. Sift flour, ba...  \n",
      "3  In a blender, add the ingredients for the spic...  \n"
     ]
    }
   ],
   "source": [
    "# List all recipes starting with the letter 'a'\n",
    "a_recipes_df = recipe_collector.list_by_first_letter(\"a\")\n",
    "print(\"Recipes starting with 'a':\")\n",
    "print(a_recipes_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe details for ID 52772:\n",
      "{'id': '52772', 'name': 'Teriyaki Chicken Casserole', 'area': 'Japanese', 'category': 'Chicken', 'instructions': 'Preheat oven to 350° F. Spray a 9x13-inch baking pan with non-stick spray.\\r\\nCombine soy sauce, ½ cup water, brown sugar, ginger and garlic in a small saucepan and cover. Bring to a boil over medium heat. Remove lid and cook for one minute once boiling.\\r\\nMeanwhile, stir together the corn starch and 2 tablespoons of water in a separate dish until smooth. Once sauce is boiling, add mixture to the saucepan and stir to combine. Cook until the sauce starts to thicken then remove from heat.\\r\\nPlace the chicken breasts in the prepared pan. Pour one cup of the sauce over top of chicken. Place chicken in oven and bake 35 minutes or until cooked through. Remove from oven and shred chicken in the dish using two forks.\\r\\n*Meanwhile, steam or cook the vegetables according to package directions.\\r\\nAdd the cooked vegetables and rice to the casserole dish with the chicken. Add most of the remaining sauce, reserving a bit to drizzle over the top when serving. Gently toss everything together in the casserole dish until combined. Return to oven and cook 15 minutes. Remove from oven and let stand 5 minutes before serving. Drizzle each serving with remaining sauce. Enjoy!', 'soy sauce': '3/4 cup', 'water': '1/2 cup', 'brown sugar': '1/4 cup', 'ground ginger': '1/2 teaspoon', 'minced garlic': '1/2 teaspoon', 'cornstarch': '4 Tablespoons', 'chicken breasts': '2', 'stir-fry vegetables': '1 (12 oz.)', 'brown rice': '3 cups', '': '', None: None}\n"
     ]
    }
   ],
   "source": [
    "# Lookup recipe details by ID 52772\n",
    "recipe_52772_data = recipe_collector.lookup_by_id(52772)\n",
    "print(\"Recipe details for ID 52772:\")\n",
    "print(recipe_52772_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random recipe:\n",
      "{'id': '52982', 'name': 'Spaghetti alla Carbonara', 'area': 'Italian', 'category': 'Pasta', 'instructions': 'STEP 1\\r\\nPut a large saucepan of water on to boil.\\r\\n\\r\\nSTEP 2\\r\\nFinely chop the 100g pancetta, having first removed any rind. Finely grate 50g pecorino cheese and 50g parmesan and mix them together.\\r\\n\\r\\nSTEP 3\\r\\nBeat the 3 large eggs in a medium bowl and season with a little freshly grated black pepper. Set everything aside.\\r\\n\\r\\nSTEP 4\\r\\nAdd 1 tsp salt to the boiling water, add 350g spaghetti and when the water comes back to the boil, cook at a constant simmer, covered, for 10 minutes or until al dente (just cooked).\\r\\n\\r\\nSTEP 5\\r\\nSquash 2 peeled plump garlic cloves with the blade of a knife, just to bruise it.\\r\\n\\r\\nSTEP 6\\r\\nWhile the spaghetti is cooking, fry the pancetta with the garlic. Drop 50g unsalted butter into a large frying pan or wok and, as soon as the butter has melted, tip in the pancetta and garlic.\\r\\n\\r\\nSTEP 7\\r\\nLeave to cook on a medium heat for about 5 minutes, stirring often, until the pancetta is golden and crisp. The garlic has now imparted its flavour, so take it out with a slotted spoon and discard.\\r\\n\\r\\nSTEP 8\\r\\nKeep the heat under the pancetta on low. When the pasta is ready, lift it from the water with a pasta fork or tongs and put it in the frying pan with the pancetta. Don’t worry if a little water drops in the pan as well (you want this to happen) and don’t throw the pasta water away yet.\\r\\n\\r\\nSTEP 9\\r\\nMix most of the cheese in with the eggs, keeping a small handful back for sprinkling over later.\\r\\n\\r\\nSTEP 10\\r\\nTake the pan of spaghetti and pancetta off the heat. Now quickly pour in the eggs and cheese. Using the tongs or a long fork, lift up the spaghetti so it mixes easily with the egg mixture, which thickens but doesn’t scramble, and everything is coated.\\r\\n\\r\\nSTEP 11\\r\\nAdd extra pasta cooking water to keep it saucy (several tablespoons should do it). You don’t want it wet, just moist. Season with a little salt, if needed.\\r\\n\\r\\nSTEP 12\\r\\nUse a long-pronged fork to twist the pasta on to the serving plate or bowl. Serve immediately with a little sprinkling of the remaining cheese and a grating of black pepper. If the dish does get a little dry before serving, splash in some more hot pasta water and the glossy sauciness will be revived.', 'Spaghetti': '320g', 'Egg Yolks': '6', 'Salt': 'As required', 'Bacon': '150g', 'Pecorino': '50g', 'Black Pepper': 'As required', '': ' '}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get a random recipe\n",
    "random_recipe_data = recipe_collector.get_random_meal()\n",
    "print(\"Random recipe:\")\n",
    "print(random_recipe_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meal categories:\n",
      "    id           name                                        description\n",
      "0    1           Beef  Beef is the culinary name for meat from cattle...\n",
      "1    2        Chicken  Chicken is a type of domesticated fowl, a subs...\n",
      "2    3        Dessert  Dessert is a course that concludes a meal. The...\n",
      "3    4           Lamb  Lamb, hogget, and mutton are the meat of domes...\n",
      "4    5  Miscellaneous  General foods that don't fit into another cate...\n",
      "5    6          Pasta  Pasta is a staple food of traditional Italian ...\n",
      "6    7           Pork  Pork is the culinary name for meat from a dome...\n",
      "7    8        Seafood  Seafood is any form of sea life regarded as fo...\n",
      "8    9           Side  A side dish, sometimes referred to as a side o...\n",
      "9   10        Starter  An entrée in modern French table service and t...\n",
      "10  11          Vegan  Veganism is both the practice of abstaining fr...\n",
      "11  12     Vegetarian  Vegetarianism is the practice of abstaining fr...\n",
      "12  13      Breakfast  Breakfast is the first meal of a day. The word...\n",
      "13  14           Goat  The domestic goat or simply goat (Capra aegagr...\n"
     ]
    }
   ],
   "source": [
    "# List all meal categories\n",
    "categories_data = recipe_collector.list_all_categories()\n",
    "print(\"Meal categories:\")\n",
    "print(categories_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Web Scraping <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "> For this part 3, I chose the Katz School Staff Website page to demonstrate my ability to implement web scraping skills using Python. The primary objective was to extract staff information from the Katz School's \"Staff\" web page, including names, titles, email addresses, phone numbers, and office locations. This report outlines the methods used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "> This part involved several key steps:\n",
    "\n",
    "> * Data Collection: Utilizing Python's requests library to retrieve the HTML content of the Katz School's staff web page.\n",
    "\n",
    "> * Data Parsing: Employing the BeautifulSoup library to parse the retrieved HTML content and extract relevant information.\n",
    "\n",
    "> * Data Organization: Structuring the extracted data into a readable and analyzable format using Python's pandas library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology**\n",
    "> * Web Scraping\n",
    "Fetching Web Content: The requests library was used to make an HTTP GET request to the Katz School’s staff web page. The status code of the response was checked to ensure successful retrieval of the page.\n",
    "\n",
    "> * HTML Parsing: BeautifulSoup was utilized to parse the HTML content. The specific HTML elements containing the desired staff information were located and extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Extraction**\n",
    "\n",
    "> * Names, Titles, Emails, Phone Numbers, and Offices: Regular expressions and string manipulation techniques were used to extract and clean the data. For instance, email addresses were validated using a regular expression pattern.\n",
    "\n",
    "> * Error Handling: Care was taken to handle missing or irregular data, such as staff members without email addresses or phone numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paul Russo</td>\n",
       "      <td>Vice Provost and Dean \\nProfessor of Data Sci...</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Office of the Dean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Ross</td>\n",
       "      <td>Assistant Dean for Academic Programs and Depu...</td>\n",
       "      <td>aaron.ross2@yu.edu</td>\n",
       "      <td>646-592-4148</td>\n",
       "      <td>Office of the Dean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sofia Binioris</td>\n",
       "      <td>Director of Communications and Strategic Init...</td>\n",
       "      <td>sofia.binioris@yu.edu</td>\n",
       "      <td>645-592-4719</td>\n",
       "      <td>Office of the Dean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jackie Hamilton</td>\n",
       "      <td>Executive Director of Enrollment Management a...</td>\n",
       "      <td>jackie.hamilton@yu.edu</td>\n",
       "      <td>646-787-6194</td>\n",
       "      <td>Office of the Dean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pamela Rodman</td>\n",
       "      <td>Director of Finance and Administration</td>\n",
       "      <td>pamela.rodman@yu.edu</td>\n",
       "      <td>NA</td>\n",
       "      <td>Office of the Dean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name                                              title  \\\n",
       "0       Paul Russo   Vice Provost and Dean \\nProfessor of Data Sci...   \n",
       "1       Aaron Ross   Assistant Dean for Academic Programs and Depu...   \n",
       "2   Sofia Binioris   Director of Communications and Strategic Init...   \n",
       "3  Jackie Hamilton   Executive Director of Enrollment Management a...   \n",
       "4    Pamela Rodman             Director of Finance and Administration   \n",
       "\n",
       "                    email         phone              office  \n",
       "0                      NA            NA  Office of the Dean  \n",
       "1      aaron.ross2@yu.edu  646-592-4148  Office of the Dean  \n",
       "2   sofia.binioris@yu.edu  645-592-4719  Office of the Dean  \n",
       "3  jackie.hamilton@yu.edu  646-787-6194  Office of the Dean  \n",
       "4    pamela.rodman@yu.edu            NA  Office of the Dean  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import the necessary class from my package\n",
    "\n",
    "from my_webdatapack.katz_staff_scraper import KatzStaffScraper\n",
    "\n",
    "# URL of the Katz School's staff web page\n",
    "url = \"https://www.yu.edu/katz/staff\"\n",
    "\n",
    "# Create a KatzStaffScraper instance\n",
    "scraper = KatzStaffScraper(url)\n",
    "\n",
    "# Scrape the data\n",
    "scraper.fetch_page_content()\n",
    "scraper.parse_html()\n",
    "names, titles, emails, phone_nums, offices = scraper.extract_staff_details()\n",
    "\n",
    "# Create a DataFrame\n",
    "staff_df = scraper.create_dataframe(names, titles, emails, phone_nums, offices)\n",
    "\n",
    "# Display the DataFrame\n",
    "staff_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis of Part 3 data <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Import the Analysis Module:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=staff_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmy_webdatapack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkatz_staff_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KatzStaffAnalysis\n\u001b[0;32m      3\u001b[0m staff_analysis \u001b[38;5;241m=\u001b[39m KatzStaffAnalysis(df)\n",
      "File \u001b[1;32m~\\Documents\\src\\my_webdatapack\\katz_staff_analysis.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKatzStaffAnalysis\u001b[39;00m(df):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    A class for analyzing staff data from the Katz School.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    Attributes:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    df (pandas.DataFrame): A DataFrame containing staff information.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtitle_distribution\u001b[39m(df):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from my_webdatapack.katz_staff_analysis import KatzStaffAnalysis\n",
    "staff_analysis = KatzStaffAnalysis(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title_distribution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m analysis \u001b[38;5;241m=\u001b[39m title_distribution(staff_df)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(titles_count)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'title_distribution' is not defined"
     ]
    }
   ],
   "source": [
    "analysis = title_distribution(staff_df)\n",
    "print(titles_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
